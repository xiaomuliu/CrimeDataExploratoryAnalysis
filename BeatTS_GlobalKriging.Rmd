---
title: "BeatTS_GlobalKriging"
date: "August 27, 2015"
output: html_document
---

```{r load-crimedata,echo=FALSE,message=FALSE,cache=TRUE}
setwd("/Users/xiaomuliu/CrimeProject/SpatioTemporalModeling/ExploratoryAnalysis/CrimeDataSTAnalysis2/")
source("importCrimeData.R")
filePath <- "/Users/xiaomuliu/CrimeProject/SpatioTemporalModeling/ExploratoryAnalysis/DataPortal/"
fileName <- "VIOLENTCRIME_01_14.csv"
CrimeData <- importCrimeData(filePath,fileName)
row.names(CrimeData) <- NULL
```

```{r load-shapefile,echo=FALSE,warning=FALSE,message=FALSE,cache=TRUE}
library(rgdal)
shapefilePath.new <- "/Users/xiaomuliu/CrimeProject/SpatioTemporalModeling/CPDShapeFiles/new/"
beat_new.rg <- readOGR(paste0(shapefilePath.new,"cpd_beats"), "cpd_beats")
district_new.rg <- readOGR(paste0(shapefilePath.new, "cpd_districts"),"cpd_districts")
# centroids 
Crd.beat <- coordinates(beat_new.rg)
Crd.district <- coordinates(district_new.rg)
```

The beat and district numbers in the data were re-assigned by finding in which new beat/district polygon the point falls and then label that beat/district number. Therefore all violent crime beat and district records have a unified reference which is the new CPD beat/district map.

```{r rearrange-data,echo=FALSE,warning=FALSE,message=FALSE,cache=TRUE}
## Match old and new beat 
# Matching the old beat records and the new one by finding which new beat polygon the point falls in
# and then re-assign that beat number to that crime record. 
# Therefore all crime beat records have a unified reference which is the new beat map.
source("DataPolyMatching.R")
Match1 <- DataMatching2(CrimeData,beat_new.rg,area="BEAT")
CrimeData <- Match1$CrimeData
Match2 <- DataMatching2(CrimeData,district_new.rg,area="DISTRICT")
CrimeData <- Match2$CrimeData

## Aggregated by "beat" and add 'holiday' attribute
source("HolidayChart.R")
CrimeData.beat_day <- aggregate(INC_CNT~BEAT+DISTRICT+DATEOCC+YEAR+MONTH+DOW,data=CrimeData, FUN=sum, na.rm=TRUE)
CrimeData.beat_day <- CrimeData.beat_day[order(CrimeData.beat_day$DATEOCC),]
CrimeData.beat_day$DOW <- factor(CrimeData.beat_day$DOW, levels=c("Sun","Mon","Tue","Wed","Thu","Fri","Sat"))
CrimeData.beat_day$HOLIDAY <- sapply(CrimeData.beat_day$DATEOCC,FUN=holidays)
CrimeData.beat_day$HOLIDAY <- factor(CrimeData.beat_day$HOLIDAY)

CrimeData$HOLIDAY <- sapply(CrimeData$DATEOCC,FUN=holidays)
CrimeData$HOLIDAY <- factor(CrimeData$HOLIDAY)
```

```{r construct-panel-data,echo=FALSE,warning=FALSE,message=FALSE,cache=TRUE}
## Create a full panel (of size "number of beats * number of dates") data frame and an corresponding STFDF
beat_template.spdf <- beat_new.rg
# remove some useless/redundant attributes
beat_template.spdf@data$SECTOR <- NULL
beat_template.spdf@data$BEAT <- NULL
beat_template.spdf@data$BEAT_NUM <- NULL
# add an attribute INC_CNT
beat_template.spdf@data$INC_CNT <- rep(NA,nrow(beat_template.spdf@data))

source("ConstructSTData.R")
STdata.beat <- ConstructArealSTData(CrimeData.beat_day,beat_template.spdf,Crd.beat,area="BEAT") 
CrimeData_beat_day.stfdf <-STdata.beat$CrimeData.stfdf 
CrimeData.beat_day <- STdata.beat$CrimeData 

# Add corresponding district numbers for each beat
inDistrict <- aggregate(.~BEAT_NUMBE,data=beat_template.spdf@data[,c("DISTRICT","BEAT_NUMBE")],FUN=function(x){x[1]})
names(inDistrict)[names(inDistrict)=="BEAT_NUMBE"] <- "BEAT"
inDistrict$DISTRICT <- factor(inDistrict$DISTRICT)
levels(inDistrict$DISTRICT) <- levels(CrimeData$DISTRICT)

CrimeData_beat_day.stfdf@data$DISTRICT <- rep(NA,nrow(CrimeData_beat_day.stfdf@data))
for (i in 1:nrow(inDistrict)){
  CrimeData.beat_day$DISTRICT[CrimeData.beat_day$BEAT==inDistrict$BEAT[i]] <- inDistrict$DISTRICT[i]
  CrimeData_beat_day.stfdf@data$DISTRICT[CrimeData_beat_day.stfdf@data$BEAT==inDistrict$BEAT[i]] <- inDistrict$DISTRICT[i]
}
```

```{r load-weather-data,echo=FALSE,warning=FALSE,message=FALSE,cache=TRUE}
## Load weather data
source("WeatherDataFunctions.R")
WeatherFilePath <- "/Users/xiaomuliu/CrimeProject/SpatioTemporalModeling/ExploratoryAnalysis/WeatherData/"
startDate="01/01/2001"
endDate="12/31/2014"
filename.daily <- paste(WeatherFilePath,'WeatherData_Daily_',as.character(as.Date(startDate, "%m/%d/%Y")),
                        '_',as.character(as.Date(endDate, "%m/%d/%Y")),'.csv',sep='')
WeatherData.daily <- read.csv(filename.daily)
WeatherData.daily$Date <- as.Date(WeatherData.daily$Date)
WeatherData.daily_diff <- DailyWeatherDiff(WeatherData.daily)
```

```{r fit-beat-trend,echo=FALSE,warning=FALSE,message=FALSE,cache=TRUE}
## Fit a temporal model which is specified below for every district
require(MASS)
require(glmnet)
require(dummies)
require(spacetime)
require(doMC)
registerDoMC(cores=4)

beatList <- sort(unique(CrimeData.beat_day$BEAT))
districtList <- sort(unique(CrimeData.beat_day$DISTRICT))
NumBeat <- length(beatList)
NumDistrict <- length(districtList)
district_NumBeat <- aggregate(BEAT~DISTRICT,data=CrimeData.beat_day,FUN=function(x){length(unique(x))})
names(district_NumBeat) <- c("DISTRICT","NumBeat")

# First 'trendLen' instances work as buffering data
trendLen <- 730
CrimeData.buffer <- CrimeData.beat_day[1:NumBeat*trendLen,]
CrimeData.nonbuffer <- CrimeData.beat_day[(NumBeat*trendLen+1):nrow(CrimeData.beat_day),]
CrimeData.nonbuffer$TStrend <- rep(NA,nrow(CrimeData.nonbuffer))
CrimeData.nonbuffer$TSdetrendRes <- rep(NA,nrow(CrimeData.nonbuffer))
WeatherData.nonbuffer <- WeatherData.daily[(trendLen+1):nrow(WeatherData.daily),]
WeatherDataDiff.nonbuffer <- WeatherData.daily_diff[(trendLen-1):nrow(WeatherData.daily_diff),] 

## Predict trend and get residuals for each district (beat)
source("TimeSeriesFunction.R")
for (i in beatList){
  CrimeData.beat <- subset(CrimeData.beat_day,BEAT==i,select=c("DATEOCC","DOW","MONTH","HOLIDAY","INC_CNT"))
  # Smooth out holiday cases:
  CrimeData.beat$INC_CNT_s <- SmoothHoliday(CrimeData.beat)
  
  Trend <- PredictTrend(CrimeData.beat,trendLen,nlfit="IRLS")
  CrimeData.nonbuffer$TStrend[CrimeData.nonbuffer$BEAT==i] <- Trend  
  CrimeData.nonbuffer$TSdetrendRes[CrimeData.nonbuffer$BEAT==i] <- CrimeData.nonbuffer$INC_CNT[CrimeData.nonbuffer$BEAT==i]-Trend
}
```

The evaluation date starts from 2014-08-01 and ends on 2014-08-04. The time series training uses 12-year data before the corresponding testing date.   
```{r kriging-setting,echo=FALSE,warning=FALSE,message=FALSE,cache=TRUE}
library(gstat)
library(xts)
library(raster)

# Evaluation periods
startDate.eval = as.Date("2014-08-01")
endDate.eval <- as.Date("2014-08-04")
dateSeq.eval <- seq.Date(startDate.eval,endDate.eval,by=1)

CrimeData.eval <- subset(CrimeData.nonbuffer,DATEOCC %in% dateSeq.eval)
CrimeData.eval$TSresPred <- rep(NA,nrow(CrimeData.eval))

# Input arguments for time series regression
glm <- "gaussian"
varSet <- c("DOW","weather","weatherdiff")
standardize <- "minmax"
Windowing <- FALSE
nlambda <- 20
Ntrain <- 365*12
winSize <- 90
winNum <- 12
parallel <- TRUE

# spatial separation distance up to which point pairs are included in semivariance estimates
# the length of the diagonal of the box spanning the data is divided by a certain number
cutoff <- 15000
width <- 500
vgm.prodsum <- vgmST("productSum",space=vgm(psill=0.5,"Sph",range=5000, nugget=0.5),
                     time=vgm(psill=0.07,"Sph",range=1,nugget=0.5), sill=1, nugget=0.5)
Nvario <- 365*2

# The spatial interpolation is done over a 'interpx*interpy' regular grid within the city border.
# spatial interpolation grid
ncell.x <- 120
ncell.y <- 160
X_range <- beat_new.rg@bbox[1,]
Y_range <- beat_new.rg@bbox[2,]
grd.full <- expand.grid(list(X_COORD=seq(X_range[1],X_range[2],length.out=ncell.x),
                             Y_COORD=seq(Y_range[1],Y_range[2],length.out=ncell.y)))
coordinates(grd.full) = ~X_COORD+Y_COORD # convert to SpatialPoints
proj4string(grd.full) <- proj4string(beat_new.rg)

grdInCity <- over(grd.full,as(beat_new.rg,"SpatialPolygons"))

grd.beat_s <- grd.full[!is.na(grdInCity)]
grd.beat_s <- SpatialPixels(grd.beat_s)

krigeST.df <- data.frame(X_COORD=rep(grd.beat_s@coords[,1],length(dateSeq.eval)),
                         Y_COORD=rep(grd.beat_s@coords[,2],length(dateSeq.eval)),
                         DATEOCC=rep(dateSeq.eval,each=nrow(grd.beat_s@coords)),
                         krigePred=rep(NA,nrow(grd.beat_s@coords)*length(dateSeq.eval)))
```

The time series models were applied to each beat. The residuals later would be used to get empirical variogram.

```{r beatTS-globalKriging,echo=FALSE,warning=FALSE,message=FALSE,cache=TRUE}
## Predict detrending residuals for each beat (by averaging district residuals)
# and then do Kriging based on beat-level spatio-temporal (global) variograms of temporal model errors

for (i in 1:length(dateSeq.eval)){
  # pinpoint the training time range
  d <- dateSeq.eval[i]
  startDate.train <- d-Ntrain
  endDate.train <- d-1
  dateSeq.train <- seq.Date(startDate.train,endDate.train,by=1)
  
  CrimeData.test <- subset(CrimeData.nonbuffer,DATEOCC==d)
  WeatherData.test <- subset(WeatherData.nonbuffer,Date==d)
  WeatherDataDiff.test <- subset(WeatherDataDiff.nonbuffer,Date==d)
  
  if (Windowing){
    dateWindow <- HistDateWindows(dateSeq.train,d,windowSize=winSize,windowNum=winNum,interval=365.25,dir="backward")
    CrimeData.train <- subset(CrimeData.nonbuffer,DATEOCC %in% dateWindow$histDates)
    WeatherData.train <- subset(WeatherData.nonbuffer,Date %in% dateWindow$histDates)
    WeatherDataDiff.train <- subset(WeatherDataDiff.nonbuffer,Date %in% dateWindow$histDates)
  }
  else{
    # use all training data
    CrimeData.train <- subset(CrimeData.nonbuffer,DATEOCC %in% dateSeq.train)
    WeatherData.train <- subset(WeatherData.nonbuffer, Date %in% dateSeq.train)
    WeatherDataDiff.train <- subset(WeatherDataDiff.nonbuffer, Date %in% dateSeq.train)
  }
  
  CrimeData.train$TSresPred <- rep(NA,nrow(CrimeData.train))
  
  for (j in beatList){
    # beat 3100 has too few samples
    if (j=="3100"){
      CrimeData.eval$TSresPred[CrimeData.eval$BEAT==j&CrimeData.eval$DATEOCC==d] <- 0
      CrimeData.train$TSresPred[CrimeData.eval$BEAT==j] <- 0
      next
    }
    
    #Combine training data of all variables     
    CrimeData.train_beat <- subset(CrimeData.train,BEAT==j,select=c("DATEOCC","DOW","MONTH","INC_CNT","TStrend","TSdetrendRes"))
    CrimeData.test_beat <- subset(CrimeData.test,BEAT==j,select=c("DATEOCC","DOW","MONTH","INC_CNT","TStrend","TSdetrendRes"))  
    selectData.train <- VariableSet(varSet,CrimeData.train_beat,WeatherData.train,WeatherDataDiff.train,glm)
    selectData.test <- VariableSet(varSet,CrimeData.test_beat,WeatherData.test,WeatherDataDiff.test,glm)
    
    X.train_raw <- selectData.train$X
    y.train <- selectData.train$y   
    X.test_raw <- selectData.test$X
    y.test <- selectData.test$y
    
    scaling.train <- Standardization(X.train_raw,X.train_raw,standardize,varSet,glm)    
    scaling.test <- Standardization(X.train_raw,X.test_raw,standardize,varSet,glm)
    X.train <- scaling.train$scaledData
    X.test <- scaling.test$scaledData
    scalingflag <- scaling.test$flag
    
    if (length(unique(y.train))<3){
      # if there is too less variation in the response, the 'cv.glmnet' will have trouble generating the lambda sequence 
      y_hat.test <- median(y.train)
      y_hat.train <- rep(median(y.train),length(y.train))
    }
    else{
      cvfit <- cv.glmnet(as.matrix(X.train),as.vector(y.train),family=glm,standardize=scalingflag,nlambda=nlambda,parallel=parallel)   
      fit.lasso <- glmnet(as.matrix(X.train),as.vector(y.train),family=glm,lambda=cvfit$lambda.min,standardize=scalingflag)    
      
      y_hat.test <- predict(fit.lasso,newx=as.matrix(X.test),type="response")
      y_hat.train <- predict(fit.lasso,newx=as.matrix(X.train),type="response")     
    } 
    CrimeData.eval$TSresPred[CrimeData.eval$BEAT==j&CrimeData.eval$DATEOCC==d] <- y_hat.test
    CrimeData.train$TSresPred[CrimeData.train$BEAT==j] <- y_hat.train
  }
  CrimeData.train$TSpred <- CrimeData.train$TSresPred+CrimeData.train$TStrend
  CrimeData.train$TSerr <- CrimeData.train$INC_CNT-CrimeData.train$TSpred  
  
  # Estimate global variograms and do Kriging prediction
  # use only recent *Nvairo* samples
  if (nrow(CrimeData.train)/NumBeat > Nvario){
    CrimeData.train <- CrimeData.train[(nrow(CrimeData.train)-Nvario*NumBeat+1):nrow(CrimeData.train),]
  }
  
  # create a global training STFDF
  CrimeData.train_stfdf <- ConstructSTData(CrimeData.train,beat_template.spdf,Crd.beat,area="BEAT")
  
  stVgm.beat <- variogramST(TSerr~1,CrimeData.train_stfdf[, ,"TSerr"],cutoff=cutoff,width=width,
                            tlags=0:14,assumeRegular=TRUE,progress=FALSE,na.omit=TRUE)
  # extractPar(vgm.prodsum)
  # parameter order: space sill, space range, time sill, time range, sill, nugget 
  vgm.prodsum_fit <- fit.StVariogram(stVgm.beat,vgm.prodsum,fit.method=6,method="L-BFGS-B",
                                     lower=c(0.01,1000,0.01,1,0.1,0.01),upper=c(10,30000,10,14,10,5))
  
  # Predict next day's violent crime distribution.
  # set up prediction grid
  # temporal grid 
  grd.beat_t <- xts(1:1,order.by=seq(d,d,by=1))
  # spatio-temporal grid
  grd.beat_st <- STF(grd.beat_s,grd.beat_t,endTime=as.POSIXct(d+1))
  
  attr(vgm.prodsum_fit, "temporal unit") <- "days"
  predST.beat <- krigeST(TSerr~1, data=CrimeData.train_stfdf[,(length(CrimeData.train_stfdf@time)-13)
                                                             :length(CrimeData.train_stfdf@time),"TSerr"], 
                         newdata=grd.beat_st, modelList=vgm.prodsum_fit, computeVar=F, progress=F)
  
  predST.df <- as.data.frame(predST.beat)
  predST.df$endTime <- NULL
  predST.df$sp.ID <- NULL
  predST.df$timedata <- NULL
  names(predST.df)[1:ncol(predST.df)] <- c("X_COORD","Y_COORD","DATEOCC","krigePred")
  
  krigeST.df[krigeST.df$DATEOCC==d,] <- predST.df
}
```

Variogram models were estimated from recent two-year beat-level residual data. All the beats over the entire city were paired up to get a global empirical variogram. We use only two year data and consider the time intervals up to 14 days to relieve computation burden. 

Three commonly used spatio-temporal covariance models are namely, 

the metric model:
$$
C_{st}(h_s,h_t) = C(a^2|h_s|^2+b^2h_t^2)
$$
the separable (product) model: 
$$
C_{st}(h_s,h_t) = C_s(h_s)C_t(h_t)
$$
the product-sum model:
$$
C_{st}(h_s,h_t) = k_1C_s(h_s)C_t(h_t)+k_2C_s(h_s)+k_3C_t(h_t)
$$

Utilizing the relation $\gamma(h)=C(0)-C(h)$, we can write these covariance expressions in terms of variogram, for example,

the separable model:
$$
\gamma_{st}(h_s,h_t) = C_t(0)\gamma_s(h_s)+C_s(0)\gamma_t(h_t)-\gamma_s(h_s)\gamma_t(h_t)
$$
the product-sum model:
$$
\gamma_{st}(h_s,h_t) = [k_2+k_1C_t(0)]\gamma_s(h_s)+[k_3+k_1C_s(0)]\gamma_t(h_t)-k_1\gamma_s(h_s)\gamma_t(h_t)
$$

where the parameter $k_1,k_2,k_3$ are defined as
$$
k_1 = [C_s(0)+C_t(0)-C_{st}(0,0)]/C_s(0)C_t(0)
$$
$$
k_2 = [C_{st}(0,0)-C_t(0)]/C_s(0)
$$
$$
k_3 = [C_{st}(0,0)-C_s(0)]/C_t(0)
$$

and variogram models $\gamma(h)$ are of forms:

spherical model 
$$
\gamma(h) =
\begin{cases}
C_{0}[\frac{3h}{2a_0}-\frac{1}{2}(\frac{h}{a_0})^3], & h \leq a_0\\
C_{0}, & h > a_0
\end{cases}
$$
exponential model
$$
\gamma(h) =
C_{0}[1-\exp(-\frac{h}{a_0})]
$$
and Gaussian model
$$
\gamma(h) =
C_{0}[1-\exp(-\frac{h^2}{a_{0}^{2}})]
$$

Where $C_0$ is the nugget and $a_0$ is the range. In our experiment, we choose the product-sum model with the spherical model for both temporal and spatial terms.

One variogram and its fitting is shown below.
```{r vgm-example, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center',fig.width=6, fig.height=4.5, cache=TRUE}
# show the variogram plot of the last evaluation example
f <- plot(stVgm.beat,vgm.prodsum_fit, all=T, wireframe=T, zlab=NULL, xlab=list("distance", rot=30), 
          ylab=list("time lag", rot=-35),scales=list(arrows=F,z=list(distance=5)), colorkey=list(width=0.75))
print(f)
```

```{r aggrange-result, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Create a STFDF with full panel (of size "number of beats * number of dates") evaluation data
CrimeData.eval <- within(CrimeData.eval, TSpred <- TSresPred+TStrend)
CrimeData.eval <- within(CrimeData.eval, TSerr <- INC_CNT-TSpred)

CrimeData.eval_stfdf <- ConstructSTData(CrimeData.eval,beat_template.spdf,Crd.beat,area="BEAT")

# Map results of form data frame to spatial (pixelized) representations
beat_template.pred_spdf <- beat_template.spdf
beat_template.pred_spdf@data$TSpred <- rep(NA,nrow(beat_template.pred_spdf@data))
beat_template.pred_spdf@data$TSerr <- rep(NA,nrow(beat_template.pred_spdf@data))
beat_template.pred_spdf@data$krigePred <- rep(NA,nrow(beat_template.pred_spdf@data))

PredResults <- data.frame(matrix(ncol=10, nrow=nrow(krigeST.df)))
colnames(PredResults) <- c("X_COORD","Y_COORD","DATEOCC","DISTRICT","BEAT","INC_CNT","TSpred","TSerr","krigePred","overlayPred")
PredResults$DATEOCC <- krigeST.df$DATEOCC

for (i in 1:length(dateSeq.eval)){
  d <- dateSeq.eval[i]
  Pred.df <- as.data.frame(CrimeData.eval_stfdf[,d])
  Pred.df$DATEOCC <- rep(d,nrow(Pred.df))
  
  beatPred.spdf <- beat_template.pred_spdf
  for (j in beatList){
    Pred.beat_sub <- subset(Pred.df,BEAT==j)
    
    beatPred.spdf@data$INC_CNT[beatPred.spdf@data$BEAT_NUMBE==j] <- Pred.beat_sub$INC_CNT
    beatPred.spdf@data$TSpred[beatPred.spdf@data$BEAT_NUMBE==j] <- Pred.beat_sub$TSpred
    beatPred.spdf@data$TSerr[beatPred.spdf@data$BEAT_NUMBE==j] <- Pred.beat_sub$TSerr
  } 
    
  polysample <- over(grd.beat_s,beatPred.spdf)
  
  polysample <- subset(cbind(polysample, grd.beat_s@coords),select=-c(OBJECTID))
  names(polysample)[names(polysample)=="BEAT_NUMBE"] <- "BEAT"
  
  PredResults.sub <- subset(krigeST.df, DATEOCC==d)
  PredResults.sub <- merge(PredResults.sub,polysample,by=c("X_COORD","Y_COORD"),all=TRUE)
  PredResults.sub$krigePred.y <- NULL                                
  names(PredResults.sub)[names(PredResults.sub)=="krigePred.x"] <- "krigePred"
  PredResults.sub$overlayPred <- PredResults.sub$TSpred + PredResults.sub$krigePred
  
  PredResults[PredResults$DATEOCC==d,names(PredResults.sub)] <- PredResults.sub
}
```
Here shows an example of time series predictions' and residuals' distribution (last evaluation example).
```{r visualize-TS-result, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center',fig.width=6, fig.height=4.5, cache=TRUE}
# Plot time series predictions and the corresponding residual spatial distribution (only show the last evaluation example)
jet.colors <- colorRampPalette(c("#00007F", "blue", "#007FFF", "cyan", "#7FFF7F", "yellow", "#FF7F00", "red", "#7F0000"))
f1 <- spplot(beatPred.spdf, zcol="TSpred", col.regions=jet.colors(256),colorkey=list(width=0.5),
             main=list("Predicted beat level crime count",cex=0.75))
f2 <- spplot(beatPred.spdf, zcol="TSerr", col.regions=jet.colors(256),colorkey=list(width=0.5),
             main=list("Residual",cex=0.75))
print(f1, position=c(0, 0, 1/2, 1), more=TRUE)
print(f2, position=c(1/2, 0, 1, 1))
```
Here is the visualization of prediction results.
```{r visualize-Krige-result, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center',fig.width=8, fig.height=8, cache=TRUE}
## Visualize prediction results
# Superimpose the acutal observations
CrimeActualPts <- subset(CrimeData,DATEOCC %in% dateSeq.eval,select=c("DATEOCC","X_COORD","Y_COORD","INC_CNT"))

library(latticeExtra)
f1 <- levelplot(overlayPred~X_COORD+Y_COORD|DATEOCC, data=PredResults,col.regions=jet.colors(256),
                colorkey=list(width=0.75),xlab="X Coordinate",ylab="Y Coordinate",as.table=TRUE,
                main=list("Prediction and Actual Incident Locations",cex=0.75))
f2 <- xyplot(Y_COORD~X_COORD|DATEOCC, data=CrimeActualPts, pch=16,col="red",cex=0.3,colorkey=list(width=0.75))
print(f1+as.layer(f2))
```
The evaluation is done through sensitivity(TPR)-like comparison between prediction model,long-term density and short-term density. To illustrate the idea, we demonstrate an example below the *sensitivity* plots: first selecting 10% highest pixels (threshold = 0.9 quantile of image histogram) then count how many actual crime incidents happened in these regions. And we do in this fashion for different threshold quantiles to get the *sensitivity* plot.
```{r evaluation, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Evalutation
# percentage of points in hot regions: raster to polygons 
# (better than contour method: having closed polygons when some sides hit boundaries)
# Sensitivity(recall)-like comparison between prediction model,long-term density and short-term density
library(igraph)
library(KernSmooth)
source("EvaluationFunction.R")

r <- raster(ncol=grd.beat_s@grid@cells.dim[1],nrow=grd.beat_s@grid@cells.dim[2],
            xmn=grd.beat_s@bbox[1,1],xmx=grd.beat_s@bbox[1,2],ymn=grd.beat_s@bbox[2,1],ymx=grd.beat_s@bbox[2,2])
period.long <- 365
period.short <- 7
probset <- seq(0,1,length.out=21)
TPR.pred <- matrix(NA,nrow=length(dateSeq.eval),ncol=length(probset))
TPR.long <- matrix(NA,nrow=length(dateSeq.eval),ncol=length(probset))
TPR.short <- matrix(NA,nrow=length(dateSeq.eval),ncol=length(probset))
bw <- 1*grd.beat_s@grid@cellsize

for (i in 1:length(dateSeq.eval)){
  d <- dateSeq.eval[i]
  
  PredResults.sub <- subset(PredResults,DATEOCC==d)
  PredResults.subRaster <- rasterize(PredResults.sub[,c("X_COORD","Y_COORD")], r, 
                                     PredResults.sub$overlayPred, fun=sum)
  
  CrimeHistPts.long <- subset(CrimeData,DATEOCC>=d-period.long & DATEOCC<=d-1,select=c("X_COORD","Y_COORD","INC_CNT"))
  CrimeHistPts.short <- subset(CrimeData,DATEOCC>=d-period.short & DATEOCC<=d-1,select=c("X_COORD","Y_COORD","INC_CNT"))
  
  KDE.long <- ConstrainedKDE(CrimeHistPts.long,grd.beat_s,beat_template.spdf,bandwidth=bw,raster=r)
  KDE.long_df_inPoly <- KDE.long$KDE.df
  KDE.long_df_inPolyRaster <- KDE.long$KDE.raster
  
  KDE.short <- ConstrainedKDE(CrimeHistPts.short,grd.beat_s,beat_template.spdf,bandwidth=bw,raster=r)
  KDE.short_df_inPoly <- KDE.short$KDE.df
  KDE.short_df_inPolyRaster <- KDE.short$KDE.raster
  
  CrimeActualPts.sub <- subset(CrimeActualPts,DATEOCC==d,select=c("X_COORD","Y_COORD","INC_CNT"))
  coordinates(CrimeActualPts.sub) <- c("X_COORD", "Y_COORD") # promote to SpatialPointsDataFrame
  proj4string(CrimeActualPts.sub) <- proj4string(beat_template.spdf)
  
  for (p in 1:length(probset)){ 
    Hit.pred <- HitRate(PredResults.sub$overlayPred,PredResults.subRaster,probset[p],CrimeActualPts.sub)  
    Hit.long<- HitRate(KDE.long_df_inPoly$VALUE,KDE.long_df_inPolyRaster,probset[p],CrimeActualPts.sub)
    Hit.short <- HitRate(KDE.short_df_inPoly$VALUE,KDE.short_df_inPolyRaster,probset[p],CrimeActualPts.sub)  
    TPR.pred[i,p] <- Hit.pred$HitRate
    TPR.long[i,p] <- Hit.long$HitRate
    TPR.short[i,p] <- Hit.short$HitRate
  }   
}
```

```{r visualize-evaluation, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center',fig.width=8, fig.height=8, cache=TRUE}
par.default <- par()
par(mfrow=c(2,2),mar=c(4, 4, 3, 2),oma=c(4,0,1,0),xpd=NA)
for (i in 1:length(dateSeq.eval)){
  plot(probset,TPR.pred[i,],type='b',col='red',cex=1,pch=16,lty="solid",
       xlab="Pixel quantile of hot spots",ylab="Hit rate",main=dateSeq.eval[i],cex.main=0.75)
  lines(probset,TPR.long[i,],type='b',col='green',cex=1,pch=15,lty="dotted")
  lines(probset,TPR.short[i,],type='b',col='blue',cex=1,pch=17,lty="dashed")
}
mtext("Sensitivity",side=3,cex=1,outer=TRUE)
par(fig=c(0,1,0,1), oma=c(0,0,0,0), mar=c(0,0,0,0), new=TRUE)
plot(0, 0, type="n", bty="n", xaxt="n", yaxt="n")
legend("bottom",legend=c("prediction model","long-term density","short-term density"),
       col=c("red","green","blue"),pch=c(16,15,17),lty=c("solid","dotted","dashed"),inset = c(0,0),xpd=TRUE)
```

```{r evaluation-demo, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center',fig.width=6, fig.height=6, cache=TRUE}
# Display one example
prob <- 0.9
Hit.pred <- HitRate(PredResults.sub$overlayPred,PredResults.subRaster,prob,CrimeActualPts.sub)  
Hit.long<- HitRate(KDE.long_df_inPoly$VALUE,KDE.long_df_inPolyRaster,prob,CrimeActualPts.sub)
Hit.short <- HitRate(KDE.short_df_inPoly$VALUE,KDE.short_df_inPolyRaster,prob,CrimeActualPts.sub)  

par <- par.default
plot(PredResults.subRaster,col=jet.colors(256), main="Kriging predition",
     panel.first=grid(grd.beat_s@grid@cells.dim[2], grd.beat_s@grid@cells.dim[1],col="lightgray", lty="dotted"))
plot(Hit.pred$inPoly_poly, border="red", lwd=1.2, add=TRUE)
plot(CrimeActualPts.sub, pch=16,col="red",cex=0.5,add=TRUE)

plot(KDE.long_df_inPolyRaster,col=jet.colors(256), main="Long-term density predition",
     panel.first=grid(grd.beat_s@grid@cells.dim[2], grd.beat_s@grid@cells.dim[1],col="lightgray", lty="dotted"))
plot(Hit.long$inPoly_poly, border="red", lwd=1.2, add=TRUE)
plot(CrimeActualPts.sub, pch=16,col="red",cex=0.5,add=TRUE)

plot(KDE.short_df_inPolyRaster,col=jet.colors(256), main="Short-term density prediction",
     panel.first=grid(grd.beat_s@grid@cells.dim[2], grd.beat_s@grid@cells.dim[1],col="lightgray", lty="dotted"))
plot(Hit.short$inPoly_poly, border="red", lwd=1.2, add=TRUE)
plot(CrimeActualPts.sub, pch=16,col="red",cex=0.5,add=TRUE)
```